{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source : https://www.kaggle.com/c/nlp-getting-started/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (7613, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(r'Data\\Real_or_Not_Diaster_Tweets\\train.csv')\n",
    "test_data = pd.read_csv(r'Data\\Real_or_Not_Diaster_Tweets\\test.csv')\n",
    "print('Training data shape: ', train_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_preprocessor(text):\n",
    "    '''\n",
    "    Make text lowercase, remove text in square brackets,remove links,remove special characters\n",
    "    and remove words containing numbers.\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub(\"\\\\W\",\" \",text) # remove special chars\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text) # remove urls\n",
    "    text = re.sub('<.*?>+', '', text)  # remove html tags\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  \n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**300dim-Glove Word emebeddings file can be downloaded from [here](http://www-nlp.stanford.edu/data/glove.840B.300d.zip)**\n",
    "\n",
    "I could not able to upload the mentoined embedding file,as the size of the file is so huge (5.25GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to convert the GloVe file format to the word2vec file format. This can be done by calling the glove2word2vec() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_input_file = 'glove.840B.300d.txt'\n",
    "# word2vec_output_file = 'word2vec_300d_4m_glove.txt'\n",
    "# glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Even the converted file (in word2vec) also in size of 5.25 GB. Please execute the above code cell & you will able to convert the Glove embeddings original format to word2vec format**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once converted, the file can be loaded just like word2vec file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading glove vectors in w2v format\n"
     ]
    }
   ],
   "source": [
    "model_glove = KeyedVectors.load_word2vec_format('word2vec_300d_4m_glove.txt',binary = False)\n",
    "print(\"Finished loading glove vectors in w2v format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate: (king - man) + woman = ?\n",
    "# result = model_glove.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models with sent2vec_mean - Converting Sentence to vector by averaging the word token vectors of a given sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec_mean(text_data):\n",
    "    Dim = model_glove.get_vector('king').shape[0]\n",
    "    \n",
    "    X_vector = np.zeros((len(text_data),Dim))\n",
    "    i = 0\n",
    "    empty_vec = 0\n",
    "    for sent in tqdm(text_data):\n",
    "        sent = custom_preprocessor(sent)\n",
    "        tokens = word_tokenize(sent)\n",
    "        tokens = [t for t in tokens if t.isalpha()]\n",
    "        word_vecs = []\n",
    "        for tokens in sent:\n",
    "            try:\n",
    "                word_vecs.append(model_glove.get_vector(tokens))\n",
    "            except KeyError:\n",
    "                pass\n",
    "        if len(word_vecs) > 0:\n",
    "            word_vecs = np.array(word_vecs)\n",
    "            X_vector[i] = word_vecs.mean(axis = 0)\n",
    "        else:\n",
    "            empty_vec+=1\n",
    "        i+=1\n",
    "    \n",
    "#     print(\"Number of samples with no words found: %s / %s\" %(empty_vec, len(text_data)))\n",
    "    print(f\"Number of samples with no words found {empty_vec} out of {len(text_data)} samples\")\n",
    "    \n",
    "    return X_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7613/7613 [00:06<00:00, 1168.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples with no words found 0 out of 7613 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Xtrain = sent2vec_mean(train_data.text)\n",
    "Ytrain = train_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 3263/3263 [00:03<00:00, 1039.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples with no words found 0 out of 3263 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Xtest = sent2vec_mean(test_data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49499545, 0.50255537, 0.50724638, 0.48392555, 0.50801688])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1.0)\n",
    "scores = model_selection.cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple Logistic Regression on Counts\n",
    "clf.fit(Xtrain, Ytrain)\n",
    "\n",
    "# Submission\n",
    "sample_submission = pd.read_csv(r\"Data\\Real_or_Not_Diaster_Tweets\\sample_submission.csv\")\n",
    "sample_submission[\"target\"] = clf.predict(Xtest)\n",
    "sample_submission.to_csv(\"submission_word_vecs_lr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gives a score of 0.62886 on public dataset in Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46804326, 0.46598322, 0.48449612, 0.5257732 , 0.5092511 ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFC = RandomForestClassifier(n_estimators = 300)\n",
    "scores = model_selection.cross_val_score(RFC, Xtrain, Ytrain, cv=5, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple Logistic Regression on Counts\n",
    "RFC.fit(Xtrain, Ytrain)\n",
    "\n",
    "# Submission\n",
    "sample_submission = pd.read_csv(r\"Data\\Real_or_Not_Diaster_Tweets\\sample_submission.csv\")\n",
    "sample_submission[\"target\"] = RFC.predict(Xtest)\n",
    "sample_submission.to_csv(\"submission_word_vecs_rfc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gives a score of 0.68495 on public dataset in Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models with sent2vec_mean - Converting Sentence to vector by normalzing the word token vectors of a given sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec_norm(text_data):\n",
    "    Dim = model_glove.get_vector('man').shape[0]\n",
    "    \n",
    "    X_vector = np.zeros((len(text_data),Dim))\n",
    "    i = 0\n",
    "    empty_vec = 0\n",
    "    for sent in tqdm(text_data):\n",
    "        sent = custom_preprocessor(sent)\n",
    "        tokens = word_tokenize(sent)\n",
    "        tokens = [t for t in tokens if t.isalpha()]\n",
    "        word_vecs = []\n",
    "        for tokens in sent:\n",
    "            try:\n",
    "                word_vecs.append(model_glove.get_vector(tokens))\n",
    "            except KeyError:\n",
    "                pass\n",
    "        if len(word_vecs) > 0:\n",
    "            word_vecs = np.array(word_vecs)\n",
    "            vec_sum = word_vecs.sum(axis = 0)\n",
    "            X_vector[i] = vec_sum/np.sqrt((vec_sum ** 2).sum())\n",
    "        else:\n",
    "            empty_vec+=1\n",
    "        i+=1\n",
    "    \n",
    "#     print(\"Number of samples with no words found: %s / %s\" %(empty_vec, len(text_data)))\n",
    "    print(f\"Number of samples with no words found {empty_vec} out of {len(text_data)} samples\")\n",
    "    \n",
    "    return X_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7613/7613 [00:05<00:00, 1470.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples with no words found 0 out of 7613 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Xtrain_norm = sent2vec_norm(train_data.text)\n",
    "Ytrain = train_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 3263/3263 [00:02<00:00, 1452.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples with no words found 0 out of 3263 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Xtest_norm = sent2vec_norm(test_data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_wv = LogisticRegression(C=1.0)\n",
    "# Fitting a simple Logistic Regression on Counts\n",
    "clf_wv.fit(Xtrain_norm, Ytrain)\n",
    "\n",
    "# Submission\n",
    "sample_submission = pd.read_csv(r\"Data\\Real_or_Not_Diaster_Tweets\\sample_submission.csv\")\n",
    "sample_submission[\"target\"] = clf_wv.predict(Xtest_norm)\n",
    "sample_submission.to_csv(\"submission_word_vecs_norm_lr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.61967 - Kaggle submission score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_norm = RandomForestClassifier(n_estimators = 300)\n",
    "\n",
    "RFC.fit(Xtrain_norm, Ytrain)\n",
    "\n",
    "# Submission\n",
    "sample_submission = pd.read_csv(r\"Data\\Real_or_Not_Diaster_Tweets\\sample_submission.csv\")\n",
    "sample_submission[\"target\"] = RFC.predict(Xtest_norm)\n",
    "sample_submission.to_csv(\"submission_word_vecs_norm_rfc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.69230 - Kaggle submission score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this function creates a normalized vector for the whole sentence\n",
    "# def sent2vec_norm(s):\n",
    "#     words = str(s).lower().decode('utf-8')\n",
    "#     words = word_tokenize(words)\n",
    "#     words = [w for w in words if not w in stop_words]\n",
    "#     words = [w for w in words if w.isalpha()]\n",
    "#     M = []\n",
    "#     for w in words:\n",
    "#         try:\n",
    "#             M.append(embeddings_index[w])\n",
    "#         except:\n",
    "#             continue\n",
    "#     M = np.array(M)\n",
    "#     v = M.sum(axis=0)\n",
    "#     if type(v) != np.ndarray:\n",
    "#         return np.zeros(300)\n",
    "#     return v / np.sqrt((v ** 2).sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
